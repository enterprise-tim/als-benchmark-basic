name: High-Performance Benchmarks (Enterprise)

on:
  push:
    branches: [ performance-test ]
  schedule:
    # Run daily at 2 AM UTC for trend monitoring
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      node_versions:
        description: 'Node.js versions to test (comma-separated)'
        required: false
        default: '18.19.1,20.11.0,22.0.0,24.6.0'
      test_iterations:
        description: 'Number of test iterations for statistical significance'
        required: false
        default: '2'
      runner_type:
        description: 'Runner type for performance testing'
        required: false
        default: 'ubuntu-latest-16-cores'
        type: choice
        options:
          - ubuntu-latest-4-cores   # 4 vCPUs, 16 GB RAM, 150 GB SSD
          - ubuntu-latest-8-cores   # 8 vCPUs, 32 GB RAM, 300 GB SSD  
          - ubuntu-latest-16-cores  # 16 vCPUs, 64 GB RAM, 600 GB SSD
          - ubuntu-latest-32-cores  # 32 vCPUs, 128 GB RAM, 1200 GB SSD
          - ubuntu-latest-64-cores  # 64 vCPUs, 256 GB RAM, 2040 GB SSD

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  performance-tests:
    runs-on: ${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}
    timeout-minutes: 120  # Increased timeout for comprehensive testing
    strategy:
      matrix:
        node-version: [18.19.1, 20.11.0, 22.0.0, 24.6.0]
      fail-fast: false  # Continue testing other versions if one fails
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Display runner specifications
      run: |
        echo "=== Runner Specifications ==="
        echo "Hostname: $(hostname)"
        echo "OS: $(cat /etc/os-release | grep PRETTY_NAME)"
        echo "CPU Info:"
        lscpu | grep -E "(Model name|CPU\(s\)|Thread|Core|Socket)"
        echo ""
        echo "Memory Info:"
        free -h
        echo ""
        echo "Disk Info:"
        df -h /
        echo ""
        echo "Available entropy (randomness):"
        cat /proc/sys/kernel/random/entropy_avail
    
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Configure system for maximum performance
      run: |
        echo "Configuring system for maximum performance testing..."
        
        # Display initial system state
        echo "Initial system load:"
        uptime
        
        # Set CPU governor to performance mode (if available)
        if [ -f /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor ]; then
          echo "Current CPU governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
          # Note: On GitHub runners, we typically can't change CPU governor
          # but we can monitor it
        fi
        
        # Disable swap to prevent memory paging during tests
        sudo swapoff -a || true
        
        # Set high process priority for our tests
        sudo renice -n -10 $$
        
        # Increase file descriptor limits
        ulimit -n 65536
        
        # Clear system caches for consistent starting state
        sudo sync
        sudo echo 3 > /proc/sys/vm/drop_caches || true
        
        # Set performance-oriented environment variables
        export NODE_ENV=production
        export NODE_OPTIONS="--max-old-space-size=16384 --no-warnings"
        export UV_THREADPOOL_SIZE=256
        
        # Display final system state
        echo "System configuration completed."
        echo "Final system load:"
        uptime
        echo "Available memory:"
        free -h | grep "^Mem:"
        
    - name: Pre-test system stability check
      run: |
        echo "Checking system stability before testing..."
        
        # Wait for system to be idle
        for i in {1..30}; do
          load=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d, -f1 | xargs)
          echo "System load check $i/30: $load"
          
          # If load is low enough, proceed
          if (( $(echo "$load < 0.5" | bc -l) )); then
            echo "System is stable (load: $load)"
            break
          fi
          
          if [ $i -eq 30 ]; then
            echo "Warning: System load is high ($load) but proceeding with tests"
          fi
          
          sleep 2
        done
        
        # Final system check
        echo "=== Pre-test System Status ==="
        echo "Load average: $(uptime | awk -F'load average:' '{ print $2 }')"
        echo "Memory usage: $(free | awk '/^Mem:/ {printf "%.1f%%", $3/$2 * 100.0}')"
        echo "Disk usage: $(df / | awk 'NR==2 {printf "%s", $5}')"
    
    - name: Run high-fidelity performance benchmarks
      run: |
        mkdir -p results/enterprise
        
        # Enhanced benchmark configuration
        ITERATIONS=${{ github.event.inputs.test_iterations || 10 }}
        WARMUP_ITERATIONS=3
        COOLDOWN_SECONDS=15
        
        echo "Starting enterprise-grade performance testing..."
        echo "Configuration:"
        echo "  Test iterations: $ITERATIONS"
        echo "  Warmup iterations: $WARMUP_ITERATIONS"
        echo "  Cooldown between iterations: $COOLDOWN_SECONDS seconds"
        echo "  Node.js version: ${{ matrix.node-version }}"
        echo "  Runner type: ${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}"
        echo ""
        
        # Create version-specific results directory
        RESULT_DIR="results/enterprise/node_${{ matrix.node-version }}"
        mkdir -p "$RESULT_DIR"
        
        # Record system configuration
        cat > "$RESULT_DIR/system_config.json" << EOF
        {
          "runner_type": "${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}",
          "node_version": "${{ matrix.node-version }}",
          "test_iterations": $ITERATIONS,
          "warmup_iterations": $WARMUP_ITERATIONS,
          "cooldown_seconds": $COOLDOWN_SECONDS,
          "timestamp": "$(date -Iseconds)",
          "system_info": {
            "cpu_count": $(nproc),
            "memory_gb": $(free -g | awk '/^Mem:/ {print $2}'),
            "load_average": "$(uptime | awk -F'load average:' '{ print $2 }')",
            "kernel": "$(uname -r)",
            "cpu_model": "$(lscpu | grep 'Model name' | sed 's/Model name:[[:space:]]*//')"
          }
        }
        EOF
        
        # Warmup iterations
        echo "Performing warmup iterations..."
        for i in $(seq 1 $WARMUP_ITERATIONS); do
          echo "Warmup iteration $i/$WARMUP_ITERATIONS"
          
          node --expose-gc -e "global.gc && global.gc()" 2>/dev/null || true
          
          timeout 120 node \
            --max-old-space-size=16384 \
            --expose-gc \
            --no-compilation-cache \
            --predictable \
            --single-threaded-gc \
            src/benchmark.js > /dev/null 2>&1 || true
            
          # Clean up warmup results
          rm -f results/benchmark_*.json 2>/dev/null || true
          
          sleep 5
        done
        
        echo "Warmup completed. Starting actual benchmarks..."
        
        # Main benchmark iterations
        for i in $(seq 1 $ITERATIONS); do
          echo ""
          echo "=== Iteration $i/$ITERATIONS ==="
          
          # Pre-iteration system check
          LOAD_BEFORE=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d, -f1 | xargs)
          MEM_BEFORE=$(free | awk '/^Mem:/ {printf "%.1f", $3/$2 * 100.0}')
          
          echo "Pre-iteration system state:"
          echo "  Load: $LOAD_BEFORE"
          echo "  Memory usage: ${MEM_BEFORE}%"
          
          # Force garbage collection before each iteration
          node --expose-gc -e "
            if (global.gc) { 
              console.log('Running garbage collection...'); 
              global.gc(); 
              global.gc(); 
            }
          " 2>/dev/null || true
          
          # Brief system settling period
          sleep 3
          
          # Create iteration-specific directory
          ITER_DIR="$RESULT_DIR/iteration_$i"
          mkdir -p "$ITER_DIR"
          
          # Record iteration start time
          START_TIME=$(date +%s.%N)
          
          # Run benchmark with maximum performance flags
          echo "Running benchmark iteration $i..."
          timeout 300 node \
            --max-old-space-size=16384 \
            --expose-gc \
            --no-compilation-cache \
            --predictable \
            --single-threaded-gc \
            --trace-gc \
            src/benchmark.js > "$ITER_DIR/output.log" 2>&1
          
          BENCHMARK_EXIT_CODE=$?
          END_TIME=$(date +%s.%N)
          DURATION=$(echo "$END_TIME - $START_TIME" | bc)
          
          # Record iteration results
          if [ $BENCHMARK_EXIT_CODE -eq 0 ]; then
            echo "  âœ“ Iteration $i completed successfully in ${DURATION}s"
            
            # Move benchmark results to iteration directory
            mv results/benchmark_*.json "$ITER_DIR/" 2>/dev/null || true
            
            # Post-iteration system check
            LOAD_AFTER=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d, -f1 | xargs)
            MEM_AFTER=$(free | awk '/^Mem:/ {printf "%.1f", $3/$2 * 100.0}')
            
            # Record iteration metadata
            cat > "$ITER_DIR/iteration_metadata.json" << EOF
            {
              "iteration": $i,
              "exit_code": $BENCHMARK_EXIT_CODE,
              "duration_seconds": $DURATION,
              "system_state": {
                "before": {
                  "load_average": "$LOAD_BEFORE",
                  "memory_usage_percent": $MEM_BEFORE
                },
                "after": {
                  "load_average": "$LOAD_AFTER", 
                  "memory_usage_percent": $MEM_AFTER
                }
              },
              "timestamp_start": "$START_TIME",
              "timestamp_end": "$END_TIME"
            }
        EOF
            
            echo "  Post-iteration system state:"
            echo "    Load: $LOAD_AFTER"
            echo "    Memory usage: ${MEM_AFTER}%"
            
          else
            echo "  âœ— Iteration $i failed (exit code: $BENCHMARK_EXIT_CODE)"
          fi
          
          # Cooldown between iterations (except for the last one)
          if [ $i -lt $ITERATIONS ]; then
            echo "  Cooling down for $COOLDOWN_SECONDS seconds..."
            sleep $COOLDOWN_SECONDS
          fi
        done
        
        echo ""
        echo "All benchmark iterations completed."
    
    - name: Generate enterprise statistical analysis
      run: |
        echo "Generating comprehensive statistical analysis..."
        
        # Use enhanced statistical analysis for enterprise results
        node --max-old-space-size=8192 src/statistical-analyzer.js "results/enterprise/node_${{ matrix.node-version }}"
        
        # Generate enterprise-specific summary
        RESULT_DIR="results/enterprise/node_${{ matrix.node-version }}"
        
        echo "=== ENTERPRISE BENCHMARK SUMMARY ===" | tee "$RESULT_DIR/summary.txt"
        echo "Node.js Version: ${{ matrix.node-version }}" | tee -a "$RESULT_DIR/summary.txt"
        echo "Runner Type: ${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}" | tee -a "$RESULT_DIR/summary.txt"
        echo "Test Date: $(date -Iseconds)" | tee -a "$RESULT_DIR/summary.txt"
        echo "Total Iterations: ${{ github.event.inputs.test_iterations || 10 }}" | tee -a "$RESULT_DIR/summary.txt"
        echo "" | tee -a "$RESULT_DIR/summary.txt"
        
        # Count successful iterations
        SUCCESSFUL_ITERATIONS=$(find "$RESULT_DIR" -name "iteration_*" -type d | wc -l)
        echo "Successful Iterations: $SUCCESSFUL_ITERATIONS" | tee -a "$RESULT_DIR/summary.txt"
        
        # Show system specifications
        echo "System Specifications:" | tee -a "$RESULT_DIR/summary.txt"
        echo "  CPU Cores: $(nproc)" | tee -a "$RESULT_DIR/summary.txt"
        echo "  Memory: $(free -h | awk '/^Mem:/ {print $2}')" | tee -a "$RESULT_DIR/summary.txt"
        echo "  CPU Model: $(lscpu | grep 'Model name' | sed 's/Model name:[[:space:]]*//')" | tee -a "$RESULT_DIR/summary.txt"
        echo "" | tee -a "$RESULT_DIR/summary.txt"
        
        # Display key results if analysis file exists
        if [ -f "results/analysis/statistical_analysis.json" ]; then
          echo "Key Performance Metrics:" | tee -a "$RESULT_DIR/summary.txt"
          node -e "
            const fs = require('fs');
            const analysis = JSON.parse(fs.readFileSync('results/analysis/statistical_analysis.json', 'utf8'));
            if (analysis.detailedAnalysis && analysis.detailedAnalysis['${{ matrix.node-version }}']) {
              const nodeResults = analysis.detailedAnalysis['${{ matrix.node-version }}'];
              Object.entries(nodeResults).forEach(([benchmark, stats]) => {
                if (stats.overhead) {
                  console.log(\`  \${benchmark}:\`);
                  console.log(\`    Mean Overhead: \${stats.overhead.mean}% Â± \${stats.overhead.stddev}%\`);
                  console.log(\`    95% CI: [\${stats.overhead.confidence_interval_95.lower}%, \${stats.overhead.confidence_interval_95.upper}%]\`);
                  console.log(\`    Coefficient of Variation: \${stats.overhead.cv_percent}%\`);
                }
              });
            }
          " | tee -a "$RESULT_DIR/summary.txt"
        fi
    
    - name: Upload enterprise results
      uses: actions/upload-artifact@v4
      with:
        name: enterprise-performance-results-node-${{ matrix.node-version }}-${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}
        path: results/enterprise/
        retention-days: 90  # Keep enterprise results longer
    
    - name: Performance regression analysis (Enterprise)
      if: github.event_name == 'pull_request'
      run: |
        echo "=== Enterprise Performance Regression Analysis ==="
        echo "Comparing against baseline performance metrics..."
        
        # This would implement sophisticated regression detection
        # using historical enterprise benchmark data
        echo "Enterprise regression analysis would compare:"
        echo "  - Statistical significance of performance changes"
        echo "  - Performance distribution changes"
        echo "  - Outlier pattern changes"
        echo "  - Memory usage regression"
        echo "  - Cross-version performance impact"
        
        # Generate regression report
        RESULT_DIR="results/enterprise/node_${{ matrix.node-version }}"
        echo "Performance regression check completed" > "$RESULT_DIR/regression_analysis.txt"
        echo "Baseline comparison: [Would compare against historical data]" >> "$RESULT_DIR/regression_analysis.txt"

  aggregate-enterprise-results:
    needs: performance-tests
    runs-on: ubuntu-latest
    if: always()  # Run even if some tests failed
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all enterprise results
      uses: actions/download-artifact@v4
      with:
        path: all-enterprise-results/
        pattern: enterprise-performance-results-*
    
    - name: Generate cross-version enterprise analysis
      run: |
        echo "Generating comprehensive enterprise performance analysis..."
        
        # Install analysis dependencies
        npm ci
        
        # Run cross-version analysis
        find all-enterprise-results/ -name "*.json" -type f | head -10
        
        # Generate enterprise dashboard data
        mkdir -p enterprise-dashboard
        
        cat > enterprise-dashboard/summary.json << EOF
        {
          "test_run": {
            "timestamp": "$(date -Iseconds)",
            "runner_type": "${{ github.event.inputs.runner_type || 'ubuntu-latest-16-cores' }}",
            "trigger": "${{ github.event_name }}",
            "iterations_per_version": ${{ github.event.inputs.test_iterations || 10 }}
          },
          "node_versions": ["18.19.1", "20.11.0", "22.0.0", "24.6.0"],
          "results_available": true
        }
        EOF
        
        echo "Enterprise analysis complete. Results available in artifacts."
    
    - name: Upload aggregated enterprise results
      uses: actions/upload-artifact@v4
      with:
        name: enterprise-aggregated-performance-results
        path: |
          all-enterprise-results/
          enterprise-dashboard/
        retention-days: 180  # Keep enterprise aggregated results for 6 months
    
    - name: Deploy Dashboard to GitHub Pages (if main branch)
      if: github.ref == 'refs/heads/main'
      run: |
        # Create dashboard deployment directory
        mkdir -p dashboard-deploy
        
        # Copy dashboard HTML
        cp docs/enterprise-dashboard.html dashboard-deploy/index.html
        
        # Copy results as JSON files for dashboard to load
        cp -r all-enterprise-results/ dashboard-deploy/
        cp -r enterprise-dashboard/ dashboard-deploy/
        
        # Create a simple landing page with download links
        cat > dashboard-deploy/results.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Enterprise Results</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .download-link { 
                    display: block; 
                    margin: 10px 0; 
                    padding: 10px; 
                    background: #f0f0f0; 
                    text-decoration: none; 
                    border-radius: 5px;
                }
            </style>
        </head>
        <body>
            <h1>Enterprise Performance Results</h1>
            <p>Latest benchmark results from enterprise runners:</p>
            <a href="index.html" class="download-link">ðŸ“Š View Interactive Dashboard</a>
        </body>
        </html>
        EOF
    
    - name: Setup Pages
      if: github.ref == 'refs/heads/main'
      uses: actions/configure-pages@v4
    
    - name: Upload to GitHub Pages
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./dashboard-deploy
    
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main'
      id: deployment
      uses: actions/deploy-pages@v4
